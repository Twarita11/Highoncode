{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e565b91",
   "metadata": {},
   "source": [
    "# Crop Market Analysis and Demand Forecasting - EDA\n",
    "\n",
    "This notebook performs exploratory data analysis on crop market data to understand trends, seasonality, and factors affecting crop prices. We'll analyze synthetic data first and then apply the same analysis to real data.\n",
    "\n",
    "## Objectives\n",
    "1. Analyze market price trends and seasonality\n",
    "2. Identify key features affecting crop prices\n",
    "3. Create market indicators and ratios\n",
    "4. Visualize patterns and relationships\n",
    "5. Prepare data for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d090bd6",
   "metadata": {},
   "source": [
    "# Load and Prepare Data\n",
    "\n",
    "We'll start by loading our synthetic data and preparing it for analysis. This includes:\n",
    "1. Loading price and weather data\n",
    "2. Basic data cleaning and formatting\n",
    "3. Creating initial market indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c71b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load price and weather data\n",
    "def load_data():\n",
    "    prices = pd.read_csv('../data/raw/mandi_prices.csv', parse_dates=['date'])\n",
    "    weather = pd.read_csv('../data/raw/weather.csv', parse_dates=['date'])\n",
    "    return prices, weather\n",
    "\n",
    "prices, weather = load_data()\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Price Data Overview:\")\n",
    "print(\"-------------------\")\n",
    "print(f\"Time Range: {prices['date'].min()} to {prices['date'].max()}\")\n",
    "print(f\"Number of records: {len(prices)}\")\n",
    "print(\"\\nSample of price data:\")\n",
    "display(prices.head())\n",
    "\n",
    "print(\"\\nWeather Data Overview:\")\n",
    "print(\"---------------------\")\n",
    "print(f\"Time Range: {weather['date'].min()} to {weather['date'].max()}\")\n",
    "print(f\"Number of records: {len(weather)}\")\n",
    "print(\"\\nSample of weather data:\")\n",
    "display(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac68315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create market indicators\n",
    "def create_market_indicators(df):\n",
    "    # Sort by date\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Price changes\n",
    "    df['price_change'] = df['modal_price'].diff()\n",
    "    df['price_pct_change'] = df['modal_price'].pct_change() * 100\n",
    "    \n",
    "    # Moving averages\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'MA_{window}'] = df['modal_price'].rolling(window=window).mean()\n",
    "        df[f'price_volatility_{window}d'] = df['modal_price'].rolling(window=window).std()\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    def calculate_rsi(prices, periods=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    df['RSI'] = calculate_rsi(df['modal_price'])\n",
    "    \n",
    "    # Price momentum\n",
    "    df['momentum_5d'] = df['modal_price'].diff(5)\n",
    "    df['momentum_20d'] = df['modal_price'].diff(20)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply indicators to price data\n",
    "prices_with_indicators = create_market_indicators(prices.copy())\n",
    "\n",
    "# Display the new features\n",
    "print(\"Market Indicators Created:\")\n",
    "print(\"-------------------------\")\n",
    "display(prices_with_indicators.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed7974",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "Let's analyze the time series components of our price data:\n",
    "1. Trend\n",
    "2. Seasonality\n",
    "3. Residual noise\n",
    "4. Stationarity test\n",
    "\n",
    "This will help us understand the underlying patterns in the price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a28945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition\n",
    "def analyze_time_series(df):\n",
    "    # Set date as index\n",
    "    ts_data = df.set_index('date')['modal_price']\n",
    "    \n",
    "    # Decompose the time series\n",
    "    decomposition = seasonal_decompose(ts_data, period=30)\n",
    "    \n",
    "    # Create subplot\n",
    "    fig = make_subplots(rows=4, cols=1, subplot_titles=('Original', 'Trend', 'Seasonal', 'Residual'))\n",
    "    \n",
    "    # Add components to subplots\n",
    "    fig.add_trace(go.Scatter(x=ts_data.index, y=ts_data.values, name='Original'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=ts_data.index, y=decomposition.trend, name='Trend'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=ts_data.index, y=decomposition.seasonal, name='Seasonal'), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(x=ts_data.index, y=decomposition.resid, name='Residual'), row=4, col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(height=800, title_text=\"Time Series Decomposition\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Perform Augmented Dickey-Fuller test\n",
    "    adf_result = adfuller(ts_data.dropna())\n",
    "    print('\\nAugmented Dickey-Fuller Test:')\n",
    "    print('---------------------------')\n",
    "    print(f'ADF Statistic: {adf_result[0]}')\n",
    "    print(f'p-value: {adf_result[1]}')\n",
    "    print('Critical values:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the time series analysis\n",
    "analyze_time_series(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0508d20",
   "metadata": {},
   "source": [
    "# Market Analysis and Feature Relationships\n",
    "\n",
    "Now let's analyze the relationships between different market indicators and weather data:\n",
    "1. Correlation analysis\n",
    "2. Price volatility patterns\n",
    "3. Weather impact on prices\n",
    "4. Market momentum indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c34c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge price and weather data\n",
    "market_data = prices_with_indicators.merge(weather, on='date', how='left')\n",
    "\n",
    "# Calculate correlations\n",
    "corr_features = ['modal_price', 'MA_7', 'MA_30', 'RSI', 'price_volatility_30d', \n",
    "                 'temp_max', 'temp_min', 'precipitation', 'humidity']\n",
    "correlation_matrix = market_data[corr_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation_matrix,\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    colorscale='RdBu',\n",
    "    zmid=0\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Correlation Heatmap',\n",
    "    height=600,\n",
    "    width=800\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Display key correlations with price\n",
    "print(\"\\nCorrelations with Modal Price:\")\n",
    "print(\"------------------------------\")\n",
    "correlations = correlation_matrix['modal_price'].sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price volatility patterns\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add price and volatility\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=market_data['date'],\n",
    "    y=market_data['modal_price'],\n",
    "    name='Price',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=market_data['date'],\n",
    "    y=market_data['price_volatility_30d'],\n",
    "    name='30-day Volatility',\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "# Update layout with secondary y-axis\n",
    "fig.update_layout(\n",
    "    title='Price vs Volatility Over Time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price',\n",
    "    yaxis2=dict(\n",
    "        title='Volatility',\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate volatility statistics\n",
    "print(\"\\nVolatility Statistics:\")\n",
    "print(\"---------------------\")\n",
    "volatility_stats = market_data['price_volatility_30d'].describe()\n",
    "print(volatility_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f565e",
   "metadata": {},
   "source": [
    "# Save Processed Data for Modeling\n",
    "\n",
    "Now that we've completed our analysis, let's save the processed data with all our computed features for use in the modeling notebook. We'll also save some key statistics and findings that will guide our modeling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_path = '../data/processed/market_data_with_features.csv'\n",
    "market_data.to_csv(output_path, index=False)\n",
    "print(f\"Saved processed data to {output_path}\")\n",
    "\n",
    "# Save key statistics and findings\n",
    "analysis_results = {\n",
    "    'time_range': {\n",
    "        'start': market_data['date'].min().strftime('%Y-%m-%d'),\n",
    "        'end': market_data['date'].max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'price_stats': market_data['modal_price'].describe().to_dict(),\n",
    "    'volatility_stats': market_data['price_volatility_30d'].describe().to_dict(),\n",
    "    'key_correlations': correlation_matrix['modal_price'].to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/analysis_results.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=4)\n",
    "print(\"\\nSaved analysis results to analysis_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
